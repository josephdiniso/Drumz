<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Course Project
  | ECE, Virginia Tech | Fall 2021: ECE 4554/5554</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
  <link href="css/bootstrap.css" rel="stylesheet">
  <link rel="icon" href="drum_icon.png">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1>Drumz - Virtual Drum Simulator</h1> 
<span style="font-size: 20px; line-height: 1.5em;"><strong>Joseph DiNiso, Enzo Saba, Steven Shaumadine</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2021 ECE 4554/5554 Computer Vision: Course Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Virginia Tech</span>
<hr>

<!-- Please see <a href="http://vision.cs.utexas.edu/projects/adapted_attributes/">this</a> for an example of how to lay out the various details of your project. You may need to provide more details than this, because you will not be submitting an associated paper to accompany the webpage. So the page should be self-contained. -->

<!-- Goal -->
<h3>Abstract</h3>

Our goal is to implement a portable augmented reality drumset. Using computer vision techniques and live webcam footage, we will build a fluid drumset simulator.
<!-- One or two sentences describing the approach you took. One or two sentences on the main result you obtained. -->
<!-- figure -->
<!-- <h3>Teaser figure</h3> -->
<!-- A figure that conveys the main idea behind the project or the main application being addressed. -->
<!-- <br><br> -->
<!-- Main Illustrative Figure  -->
<!-- <div style="text-align: center;"> -->
<!-- <img style="height: 200px;" alt="" src="mainfig.png"> -->
<!-- </div> -->

<br><br>
<!-- Introduction -->
<h3>Introduction</h3>
A portable drum set can be created using computer vision to track a player’s placement and force applied to a surface by
physical drumsticks and output the corresponding sound. The output sounds depend on which surface the drum sticks impact
and the speed at which the player moves the drum sticks to calculate the impact on the surface.

<br><br>
<!-- Approach -->
<h3>Approach</h3>
Our basic approach is simple. We want to track the players’ movements with the drumsticks in the video feed and the
velocity of the drumsticks before impacting a virtual surface, or also known as one of the virtual drums. To succeed in
accurately detecting a player’s motions with the drumsticks, we would need to make some assumptions about the drumsticks
the player is using. Before loading the virtual drums program, we will require the player to disclose the color of their
drumstick to the program so that the program can distinguish the drums from the background. This gives the player the
freedom to choose among the available options for drumsticks in their immediate vicinity. The program will check to make
sure it can distinguish the drumsticks from the background before proceeding to running the program. Possible scenarios
where the program may not be able to differentiate drumsticks include the drumstick colors are not unique with respect
to the background, drumsticks are too small, etc. If the program can not differentiate the drumsticks, it will recommend
to the player to choose larger drumsticks with brighter colors that are different from the colors of its surroundings.
Assuming the player does this, the program requires the player to disclose the drumstick color again and try to detect
the drumsticks again. Should the program succeed, the player is ready to start playing the drums. The only other
assumption we will make about the player is that they are located in the center of the screen. This is because to fully
encapsulate our vision of the virtual drum, the player should be able to play anywhere without requiring much material.
<br><br>

The virtual drums themselves (the surfaces the players will strike with the drumsticks to produce a noise) are virtual
and can only be seen on screen. To an onlooker, the player would appear to be striking the air with drumsticks. The
program detects the location of the drumsticks and if they fall within the circular threshold in the video feed then the
drumsticks’ velocity is inputted to a function that will display the corresponding sound. These thresholds placed in the
video feed to reflect each of the virtual drums in the drumsets cannot be constant between video feeds unless we were to
make another assumption about the players’ location within the video feed. Instead, we will make the virtual drums’
locations in the video feed relative to the players’ distance from the camera. Should the camera detect the player is
further away, the thresholds would decrease in size and vice versa for a player that is closer to the camera. The
different surfaces produced by the program will represent different drum types (e.g. bass, snare, etc.) as seen in
figure 1 below and their locations will correspond to those of an actual drum set from the player’s point of view. The
point of view of the program can be seen in figure 2 below. The actual drum set we plan on implementing deprecates the
typical drum set for simplicity <br><br>



<img style="height: 300px; display: block; margin-left: auto; margin-right: auto; display:block" alt="" src="drums.png"><br>
<p style="text-align: center;"><i>Figure 1: Typical Drum Set</i></p> <br> <br>

<img style="height: 300px; display: block; margin-left: auto; margin-right: auto; display:block" alt=""
  src="drum-draw.png"><br>
<p style="text-align: center;"><i>The player hits the bass drum</i></p> <br> <br>


To determine when a drumstick has hit a virtual surface we will use one main detector; when the surface is occluded by
the tracked drumstick. As seen in figure 3 below, the player’s right drumstick has occluded the virtual surface of the
bass drum so a bass sound with intensity corresponding to the velocity with which the drumstick entered the surface
would be outputted. The drumsticks’ velocity is always tracked. The reading for velocity is taken by calculating the
difference between the drumsticks position during a time interval. This interval will have to be determined during
testing and will be smaller than a second. The volume of the beat will be determined by the velocity of the drumstick
once the drumstick has hit the surface. <br><br>

<img style="height: 300px; display: block; margin-left: auto; margin-right: auto; display:block" alt=""
  src="thresh.png"><br>
<p style="text-align: center;"><i>Example color thresholding</i></p> <br> <br>

<img style="height: 300px; display: block; margin-left: auto; margin-right: auto; display:block" alt=""
  src="contours.png"><br>
<p style="text-align: center;"><i>Example of contour detection</i></p> <br> <br>

<!-- Results -->
<h3>Experiments and results</h3>
Our goal for this project is to use as few external sources of code and data as possible, we wish to collect and write
as much of the project ourselves. Because we believe that the data needed for our project is fairly unique, the entirety
of data collected will be from our own recordings. We will however not be implementing all of the code used for the 
project, we will mainly be using NumPy and OpenCV for our image processing and application of computer vision algorithms 
because they run significantly faster than any Python implementation of our own doing. This is because they are written
in C++ and are compiled to be as efficient as possible. Because this is a live simulation, our code will need to run
as fast as possible, so in the case where an algorithm we implement does not run fast enough on Python we will consider
writing it in C++ and making Python bindings.<br><br>

Data will be collected by simulating intended user actions, such as banging drumsticks on the surfaces and playing
different combinations of notes. In order to have robust testing, we will make sure to test for the various different
edge cases that often occur in computer vision applications. These include dark footage, image noise, and any other sources
of error often found in image processing. <br><br> 

Successful implementation would result in smooth audio playing in response to the user’s actions (e.g. when the user hits the
surface with the drumsticks a sound is immediately played, random sounds do not play without cause, etc.). As well, we
would like to test our implementation on other people who did not develop the project and were given limited
instructions as to how it works; this is because we would like to make a user-friendly interface that does not require
much training. Other determiners of success are fast processing times, we wish to create an algorithm that can run
on a normal laptop smoothly without any visible lag. One way that we can improve this is by only processing every
3rd frame, but displaying them at 60FPS to avoid long computation times. The project will be deemed successful if a fluid 
drum set simulator can be created that is both fun, responsive, and can display various different computer vision 
algorithms such as thresholding, contour detection, motion and velocity estimation, and collision detection. 

<br><br>

<!-- Main Results Figure 
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="results.png">
</div>
<br><br>

<h3>Qualitative results</h3>
Show several visual examples of inputs/outputs of your system (success cases and failures) that help us better understand your approach.
<br><br>


<div style="text-align: center;">
<img style="height: 300px;" alt="" src="qual_results.png">
</div>
<br><br>

<h3>Conclusion</h3>
This report has described .... Briefly summarize what you have done. 
<br><br>

<h3>References</h3>
Provide a list of references to other work that supported your project.
<br><br> -->


  <hr>
  <footer> 
  <p>© Enzo Saba, Joseph DiNiso, Steven Shaumadine</p>
  </footer>
</div>
</div>

<br><br>

</body></html>